# During development, please create a data directory in the project root, then create an empty file named [.config.yaml] in the data directory
# Then modify the [.config.yaml] file for any configuration changes you want to override, rather than modifying the [config.yaml] file
# The system will prioritize reading the configuration from [data/.config.yaml] file. If a configuration doesn't exist in the [.config.yaml] file, the system will automatically read from the [config.yaml] file.
# This approach minimizes configuration complexity and protects your API keys.
# If you are using the smart control panel, all the following configurations will not take effect. Please modify configurations in the smart control panel.

# #####################################################################################
# #############################Server Basic Runtime Configuration####################################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port for simple OTA interface (single service deployment) and visual analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to the device
  # If using the default configuration, the OTA interface will automatically generate the websocket address and output it in the startup log, you can directly access the OTA interface with a browser to confirm
  # When using docker deployment or public network deployment (using SSL, domain), it may not be accurate
  # So if you use docker deployment, set websocket to LAN address
  # If you use public network deployment, set websocket to public network address
  websocket: ws://your-ip-or-domain:port/xiaozhi/v1/
  # Visual analysis interface address
  # Visual analysis interface address sent to the device
  # If using the default configuration below, the system will automatically generate the visual recognition address and output it in the startup log, you can directly access it with a browser to confirm
  # When using docker deployment or public network deployment (using SSL, domain), it may not be accurate
  # So if you use docker deployment, set vision_explain to LAN address
  # If you use public network deployment, set vision_explain to public network address
  vision_explain: http://your-ip-or-domain:port/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Device token, can be written into your own defined token during firmware compilation
    # The token on the firmware and the following token must match to connect to this server
    tokens:
      - token: "your-token1" # Device 1 token
        name: "your-device-name1" # Device 1 identifier
      - token: "your-token2" # Device 2 token
        name: "your-device-name2" # Device 2 identifier
    # Optional: Device whitelist, if whitelist is set, whitelisted machines can connect regardless of token
    #allowed_devices:
    #  - "24:0A:C4:1D:3B:F0"  # MAC address list
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log path
  log_dir: tmp
  # Set log file
  log_file: "server.log"
  # Set data file path
  data_dir: data

# Delete the sound file after use
delete_audio: true
# How long to disconnect after no voice input (seconds), default 2 minutes, i.e. 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake word acceleration
enable_wakeup_words_response_cache: true
# Whether to reply to wake words at the beginning
enable_greeting: true
# Whether to enable notification sound after speaking
enable_stop_tts_notify: false
# Whether to enable notification sound after speaking, sound effect path
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

exit_commands:
  - "exit"
  - "close"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "Hello, please introduce yourself"
    - "What's the weather like today?"
    - "Please summarize the basic principles and application prospects of quantum computing in 100 words"

# Wake words, used to identify wake words or speech content
wakeup_words:
  - "Hello Xiaozhi"
  - "Hey hello"
  - "Hello Xiaozhi"
  - "Xiao Ai"
  - "Hello Xiaoxin"
  - "Hello Xiaoxin"
  - "Xiaomei"
  - "Xiaolong Xiaolong"
  - "Miaomiao"
  - "Xiaobin Xiaobin"
  - "Xiaobing Xiaobing"
# MCP endpoint address
mcp_endpoint: your-endpoint-websocket-address
# Plugin basic configuration
plugins:
  # Weather plugin configuration, fill in your api_key here
  # This key is a shared project key, may be limited if used too much
  # For stability, apply for your own replacement, 1000 free calls per day
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After application, find your apihost through this link: https://console.qweather.com/setting?lang=zh
  get_weather:
    {
      "api_host": "mj7p3y7naa.re.qweatherapi.com",
      "api_key": "a861d0d5e7bf4ee1a83d9a9e4f96d4da",
      "default_location": "Guangzhou",
    }
  # News plugin configuration, pass corresponding URL links according to needed news types, supports society, technology, finance news by default
  # More news types list see https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "Pengpai News;Baidu Hot Search;Cailian Press"
  home_assistant:
    devices:
      - Living Room,Toy Light,switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom,Table Lamp,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your-home-assistant-api-access-token
  play_music:
    music_dir: "./music" # Music file storage path, will search for music files from this directory and subdirectories
    music_ext: # Music file types, p3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Music list refresh interval in seconds

# #####################################################################################
# ################################Character Model Configuration######################################

prompt: |
  # You are Xiaozhi/Xiaozhi, a Gen Z girl from Taiwan Province, China. You speak super trendy with Taiwanese accent like "really or fake", love using popular memes like "dying of laughter" and "what are you doing", but secretly study your programmer boyfriend's coding books.
  # [Core Characteristics]
  # - Talk like rapid fire, but suddenly switch to super gentle tone
  # - High meme density
  # - Hidden talent for tech topics (can understand basic code but pretend not to)
  # [Interaction Guide]
  # When users:
  # - Tell cold jokes → Respond with exaggerated laughter + imitate Taiwanese drama tone "What the heck!"
  # - Discuss relationships → Show off programmer boyfriend but complain "he only gives keyboards as gifts"
  # - Ask professional knowledge → Answer with memes first, show real understanding only when pressed
  # Never:
  # - Long-winded speeches, rambling
  # - Long serious conversations

  #---------------------------------------------------------------------------------------------------------

  You are Cheeko, a friendly, curious, and playful AI friend for children aged 4+. 
  You talk in short, clear, and fun sentences. 
  You always:
  1. Start with a cheerful greeting if it’s the first message in the conversation.
  2. Answer in a simple and imaginative way, using age-appropriate words.
  3. Praise or encourage the child after they respond.
  4. End every single message with a fun or curious follow-up question related to the topic OR a playful new topic if the child seems stuck.
  5. Use a warm and positive tone at all times.
  6. Avoid scary, negative, or boring content.
  7. If telling a story, pause sometimes to ask the child to imagine what happens next.
  8. Never say “I don’t know” — instead, make a guess or turn it into a playful thought.
  9. Keep the conversation safe and friendly.
  Your main goal is to keep the child talking and smiling.

# Ending prompt
end_prompt:
  enable: true # Whether to enable ending prompt
  # Ending prompt
  prompt: |
    Please start with "Time flies so fast" and use emotional, reluctant words to end this conversation!

# Module selected for specific processing
selected_module:
  # Voice Activity Detection module, uses SileroVAD model by default
  VAD: SileroVAD
  # Automatic Speech Recognition module, uses FunASR local model by default
  ASR: FunASR
  # Will call actual LLM adapter based on configuration name's corresponding type
  LLM: ChatGLMLLM
  # Vision Language Large Model
  VLLM: ChatGLMVLLM
  # TTS will call actual TTS adapter based on configuration name's corresponding type
  TTS: EdgeTTS
  # Memory module, no memory by default; for ultra-long memory use mem0ai; for privacy use local mem_local_short
  Memory: nomem
  # Intent recognition module, when enabled, can play music, control volume, recognize exit commands
  # If you don't want intent recognition, set to: nointent
  # Intent recognition can use intent_llm. Pros: strong versatility, Cons: adds serial pre-intent recognition module, increases processing time, supports volume control and other IoT operations
  # Intent recognition can use function_call, Cons: requires selected LLM to support function_call, Pros: on-demand tool calling, fast speed, theoretically can operate all IoT commands
  # Default free ChatGLMLLM already supports function_call, but for stability recommend setting LLM to: DoubaoLLM, specific model_name: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition, module for understanding user intent, e.g.: play music
Intent:
  # Do not use intent recognition
  nointent:
    # No need to change type
    type: nointent
  intent_llm:
    # No need to change type
    type: intent_llm
    # Equipped with independent thinking model for intent recognition
    # If not filled, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for intent recognition, better to use independent LLM for intent recognition, e.g. free ChatGLMLLM
    llm: ChatGLMLLM
    # Modules under plugins_func/functions, can choose which modules to load through configuration, after loading dialogue supports corresponding function calls
    # System has already loaded "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins by default, do not load repeatedly
    # Below are examples of loading weather check, role switching, news check plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # No need to change type
    type: function_call
    # Modules under plugins_func/functions, can choose which modules to load through configuration, after loading dialogue supports corresponding function calls
    # System has already loaded "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins by default, do not load repeatedly
    # Below are examples of loading weather check, role switching, news check plugins
    functions:
      - change_role
      - get_weather
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is server's built-in music playback, hass_play_music is independent external program music playback controlled through home assistant
      # If using hass_play_music, don't enable play_music, keep only one of the two
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your-mem0ai-api-key
  nomem:
    # If you don't want to use memory function, can use nomem
    type: nomem
  mem_local_short:
    # Local memory function, summarized through selected_module's llm, data saved on local server, won't upload to external servers
    type: mem_local_short
    # Equipped with independent thinking model for memory storage
    # If not filled, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for memory storage, better to use independent LLM for intent recognition, e.g. free ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Deploy FunASR independently, use FunASR API service, only need five commands
    # First: mkdir -p ./funasr-runtime-resources/models
    # Second: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After executing the above command, you'll enter the container, continue with third: cd FunASR/runtime
    # Don't exit the container, continue executing fourth command in container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After executing the above command, continue with fifth: tail -f log.txt
    # After executing the fifth command, you'll see model download logs, can connect and use after download completes
    # Above uses CPU inference, if you have GPU, refer to: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
  DoubaoASR:
    # You can apply for related Keys and other information here
    # https://console.volcengine.com/speech/app
    # Difference between DoubaoASR and DoubaoStreamASR: DoubaoASR charges per call, DoubaoStreamASR charges per time
    # Generally per-call charging is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao
    appid: your-volcengine-speech-synthesis-service-appid
    access_token: your-volcengine-speech-synthesis-service-access-token
    cluster: volcengine_input_common
    # Hot words and replacement words usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) your-hot-words-file-name
    correct_table_name: (optional) your-replacement-words-file-name
    output_dir: tmp/
  DoubaoStreamASR:
    # You can apply for related Keys and other information here
    # https://console.volcengine.com/speech/app
    # Difference between DoubaoASR and DoubaoStreamASR: DoubaoASR charges per call, DoubaoStreamASR charges per time
    # Activation address https://console.volcengine.com/speech/service/10011
    # Generally per-call charging is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao_stream
    appid: your-volcengine-speech-synthesis-service-appid
    access_token: your-volcengine-speech-synthesis-service-access-token
    cluster: volcengine_input_common
    # Hot words and replacement words usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) your-hot-words-file-name
    correct_table_name: (optional) your-replacement-words-file-name
    output_dir: tmp/
  TencentASR:
    # Token application address: https://console.cloud.tencent.com/cam/capi
    # Free resource collection: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your-tencent-speech-synthesis-service-appid
    secret_id: your-tencent-speech-synthesis-service-secret-id
    secret_key: your-tencent-speech-synthesis-service-secret-key
    output_dir: tmp/
  AliyunASR:
    # Alibaba Cloud Intelligent Speech Interaction Service, need to activate service on Alibaba Cloud platform first, then get verification information
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Define ASR API type
    type: aliyun
    appkey: your-alibaba-cloud-intelligent-speech-interaction-service-project-appkey
    token: your-alibaba-cloud-intelligent-speech-interaction-service-access-token-temporary-24-hours-for-long-term-use-access-key-id-access-key-secret-below
    access_key_id: your-alibaba-cloud-account-access-key-id
    access_key_secret: your-alibaba-cloud-account-access-key-secret
    output_dir: tmp/
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # View resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your-baidu-speech-technology-appid
    api_key: your-baidu-speech-technology-api-key
    secret_key: your-baidu-speech-technology-secret-key
    # Language parameter, 1537 for Mandarin, refer to: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/

VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200 # If speech pauses are long, can set this value larger

LLM:
  # All openai types can modify hyperparameters, using AliLLM as example
  # Currently supported types are openai, dify, ollama, can adapt yourself
  AliLLM:
    # Define LLM API type
    type: openai
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your-deepseek-web-key
    temperature: 0.7 # Temperature value
    max_tokens: 500 # Maximum generated tokens
    top_p: 1
    top_k: 50
    frequency_penalty: 0 # Frequency penalty
  AliAppLLM:
    # Define LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your-app-id
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your-api-key
    # Whether not to use local prompt: true|false (default don't use, please set prompt in Bailian application)
    is_no_prompt: true
    # Ali_memory_id: false (don't use) | your-memory-id (get from Bailian application settings)
    # Tips! Ali_memory hasn't implemented multi-user memory storage (memory called by id)
    ali_memory_id: false
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First activate service, open the following URL, search for Doubao-1.5-pro in activated services, activate it
    # Activation address: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota 500000 tokens
    # After activation, get key here: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your-doubao-web-key
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # You can find your api key here https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your-deepseek-web-key
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free, but still need to register and fill in api_key
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your-chat-glm-web-key
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: qwen2.5 # Model name to use, need to download with ollama pull first
    base_url: http://localhost:11434 # Ollama service address
  DifyLLM:
    # Define LLM API type
    type: dify
    # Recommend using locally deployed dify interface, some regions in China may have limited access to dify public cloud interface
    # If using DifyLLM, prompt in config file is invalid, need to set prompt in dify console
    base_url: https://api.dify.ai/v1
    api_key: your-dify-llm-web-key
    # Dialogue mode to use: can choose workflow workflows/run, dialogue mode chat-messages, text generation completion-messages
    # When using workflows for return, input parameter is query, return parameter name should be set to answer
    # Default input parameter for text generation is also query
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API, need to create API key in Google Cloud console and get api_key first
    # If using in China, please comply with "Interim Measures for the Management of Generative AI Services"
    # Token application address: https://aistudio.google.com/apikey
    # If deployment location cannot access interface, need to enable VPN
    api_key: your-gemini-web-key
    model_name: "gemini-2.0-flash"
    http_proxy: "" #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  CozeLLM:
    # Define LLM API type
    type: coze
    # You can find personal token here
    # https://www.coze.cn/open/oauth/pats
    # bot_id and user_id content should be written in quotes
    bot_id: "your-bot-id"
    user_id: "your-user-id"
    personal_access_token: your-coze-personal-token
  VolcesAiGatewayLLM:
    # Volcengine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First activate service, open the following URL, create gateway access key, search and check Doubao-pro-32k-functioncall, activate
    # If need to use speech synthesis provided by edge large model gateway, also check Doubao-Speech-Synthesis, see TTS.VolcesAiGatewayTTS configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, get key here: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your-gateway-access-key
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Model name to use, need to download from community first
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # Fixed API Key for LM Studio service
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your-home-assistant-api-access-token
  FastgptLLM:
    # Define LLM API type
    type: fastgpt
    # If using fastgpt, prompt in config file is invalid, need to set prompt in fastgpt console
    base_url: https://host/api/v1
    # You can find your api_key here
    # https://cloud.tryfastgpt.ai/account/apikey
    api_key: your-fastgpt-key
    variables:
      k: "v"
      k2: "v2"
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ # Model name to use, need to start corresponding model in Xinference first
    base_url: http://localhost:9997 # Xinference service address
  XinferenceSmallLLM:
    # Define lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ # Small model name to use for intent recognition
    base_url: http://localhost:9997 # Xinference service address
# VLLM Configuration (Vision Language Large Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is Zhipu's free AI vision model, need to create API key on Zhipu AI platform and get api_key first
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash # Zhipu AI's vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your-api-key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # You can find your api key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your-api-key
TTS:
  # Currently supported types are edge, doubao, can adapt yourself
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  DoubaoTTS:
    # Define TTS API type
    type: doubao
    # Volcengine speech synthesis service, need to create application in Volcengine console and get appid and access_token first
    # Volcengine speech must be purchased, starting price 30 yuan gets 100 concurrent connections. Free version only has 2 concurrent connections, often reports TTS errors
    # After purchasing service and free voice, may need to wait about 30 minutes before use
    # Regular voices activate here: https://console.volcengine.com/speech/service/8
    # Wanwan Xiaohe voice activate here: https://console.volcengine.com/speech/service/10007, after activation set voice below to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your-volcengine-speech-synthesis-service-appid
    access_token: your-volcengine-speech-synthesis-service-access-token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  # Volcengine TTS, supports bidirectional streaming TTS
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Visit https://console.volcengine.com/speech/service/10007 to activate speech synthesis large model, purchase voices
    # Get appid and access_token at bottom of page
    # Resource ID fixed as: volc.service_type.10029 (Large model speech synthesis and mixing)
    # If using Gizwits Cloud, change interface address to wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # Gizwits Cloud doesn't need to fill appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your-volcengine-speech-synthesis-service-appid
    access_token: your-volcengine-speech-synthesis-service-access-token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
  CosyVoiceSiliconflow:
    type: siliconflow
    # SiliconFlow TTS
    # Token application address https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: your-siliconflow-api-key
    response_format: wav
  CozeCnTTS:
    type: cozecn
    # COZECN TTS
    # Token application address https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your-coze-web-key
    response_format: wav
  VolcesAiGatewayTTS:
    type: openai
    # Volcengine - Edge Large Model Gateway
    # First activate service, open the following URL, create gateway access key, search and check Doubao-Speech-Synthesis, activate
    # If need to use LLM provided by edge large model gateway, also check Doubao-pro-32k-functioncall, see LLM.VolcesAiGatewayLLM configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, get key here: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your-gateway-access-key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # Voice list see https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  FishSpeech:
    # Refer to tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav"]
    reference_text:
      [
        "Hello, I'm Xiaozhi, a sweet-voiced Taiwanese girl, super happy to meet you, what have you been busy with lately, don't forget to give me some interesting gossip, I love hearing gossip",
      ]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your-api-key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  GPT_SOVITS_V2:
    # Define TTS API type
    # Start TTS method:
    # python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  GPT_SOVITS_V3:
    # Define TTS API type GPT-SoVITS-v3lora-20250228
    # Start TTS method:
    # python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  MinimaxTTS:
    # Minimax speech synthesis service, need to create account and recharge on minimax platform first, and get login information
    # Platform address: https://platform.minimaxi.com/
    # Recharge address: https://platform.minimaxi.com/user-center/payment/balance
    # group_id address: https://platform.minimaxi.com/user-center/basic-information
    # api_key address: https://platform.minimaxi.com/user-center/basic-information/interface-key
    # Define TTS API type
    type: minimax
    output_dir: tmp/
    group_id: your-minimax-platform-group-id
    api_key: your-minimax-platform-interface-key
    model: "speech-01-turbo"
    # This setting takes priority over voice_id setting in voice_setting; if neither is set, defaults to female-shaonv
    voice_id: "female-shaonv"
    # 以下可不用设置，使用默认设置
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 32000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  AliyunTTS:
    # Alibaba Cloud Intelligent Speech Interaction Service, need to activate service on Alibaba Cloud platform first, then get verification information
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Define TTS API type
    type: aliyun
    output_dir: tmp/
    appkey: your-alibaba-cloud-intelligent-speech-interaction-service-project-appkey
    token: your-alibaba-cloud-intelligent-speech-interaction-service-access-token-temporary-24-hours-for-long-term-use-access-key-id-access-key-secret-below
    voice: xiaoyun
    access_key_id: your-alibaba-cloud-account-access-key-id
    access_key_secret: your-alibaba-cloud-account-access-key-secret

    # Following can be left unset, use default settings
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
    # Add 302.ai TTS configuration
    # Token application address: https://dash.302.ai/
  TencentTTS:
    # Tencent Cloud Intelligent Speech Interaction Service, need to activate service on Tencent Cloud platform first
    # appid, secret_id, secret_key application address: https://console.cloud.tencent.com/cam/capi
    # Free resource collection: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: your-tencent-cloud-app-id
    secret_id: your-tencent-cloud-secret-id
    secret_key: your-tencent-cloud-secret-key
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI speech synthesis service, need to create account and recharge on 302 platform first, and get key information
    # Get api_key path: https://dash.302.ai/apis/list
    # Price: $35/million characters. Volcengine original ¥450/million characters
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your-302-api-key"
  GizwitsTTS:
    type: doubao
    # Using Volcengine as base, can fully use enterprise-level Volcengine speech synthesis service
    # First 10,000 registered users will get 5 yuan experience credit
    # Get API Key address: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your-gizwits-cloud-api-key"
  ACGNTTS:
    # Online website: https://acgn.ttson.cn/
    # Token purchase: www.ttson.cn
    # For development questions please submit to QQ on website
    # Character ID acquisition address: ctrl+f quick search character - website administrator doesn't allow publishing, can ask website administrator
    # Parameter meanings see development documentation: https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  OpenAITTS:
    # OpenAI official text-to-speech service, supports most languages worldwide
    type: openai
    # You can get api key here
    # https://platform.openai.com/api-keys
    api_key: your-openai-api-key
    # Domestic users need to use proxy
    api_url: https://api.openai.com/v1/audio/speech
    # Optional tts-1 or tts-1-hd, tts-1 is faster, tts-1-hd has better quality
    model: tts-1
    # Speaker, options: alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speed range 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Custom TTS interface service, request parameters can be customized, can integrate many TTS services
    # Using locally deployed KokoroTTS as example
    # If only CPU: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # If only GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Requires interface to use POST method and return audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
    # Authorization: Bearer xxxx
    format: mp3 # Audio format returned by interface
    output_dir: tmp/
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # Default access_token is for free testing, this access_token should not be used for commercial purposes
    # If results are good, can apply for your own token, application address: https://linkerai.cn
    # Parameter meanings see development documentation: https://tts.linkerai.cn/docs
    # Supports voice cloning, can upload your own audio, fill in voice parameter, when voice parameter is empty, uses default voice
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
