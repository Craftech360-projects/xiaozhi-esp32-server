# Ollama Models - Function Calling Support Guide

## üéØ Function Calling Requirements

For MCP (device control) to work, you need a model that supports **function calling / tool use**.

## ‚úÖ Models That Support Function Calling

### Llama Models:
| Model | Size | Function Calling | Speed | Recommendation |
|-------|------|------------------|-------|----------------|
| **llama3.1:8b** | 4.7GB | ‚úÖ YES | ‚ö°‚ö°‚ö° Fast | ‚úÖ **BEST for function calling** |
| **llama3.1:70b** | 40GB | ‚úÖ YES | ‚ö° Slow | ‚ùå Too large |
| **llama3.2:1b** | 1.3GB | ‚ùå NO | ‚ö°‚ö°‚ö°‚ö°‚ö° Fastest | ‚ùå No tools |
| **llama3.2:3b** | 2.0GB | ‚ùå NO | ‚ö°‚ö°‚ö°‚ö° Very Fast | ‚ùå No tools (current) |
| **llama3.3:70b** | 43GB | ‚úÖ YES | ‚ö° Slow | ‚ùå Too large |

### Other Models with Function Calling:
| Model | Size | Function Calling | Speed | Recommendation |
|-------|------|------------------|-------|----------------|
| **qwen2.5:7b** | 4.7GB | ‚úÖ YES | ‚ö°‚ö°‚ö° Fast | ‚úÖ Excellent |
| **qwen2.5:14b** | 9.0GB | ‚úÖ YES | ‚ö°‚ö° Moderate | ‚ö†Ô∏è Large |
| **qwen3:4b** | 2.5GB | ‚úÖ YES | ‚ö°‚ö°‚ö°‚ö° Very Fast | ‚úÖ **RECOMMENDED** (current) |
| **mistral:7b** | 4.1GB | ‚úÖ YES | ‚ö°‚ö°‚ö° Fast | ‚úÖ Good |
| **mixtral:8x7b** | 26GB | ‚úÖ YES | ‚ö° Slow | ‚ùå Too large |
| **gemma2:9b** | 5.5GB | ‚úÖ YES | ‚ö°‚ö°‚ö° Fast | ‚úÖ Good |
| **gemma3:1b** | 815MB | ‚ùå NO | ‚ö°‚ö°‚ö°‚ö°‚ö° Fastest | ‚ùå No tools |
| **phi3:3.8b** | 2.3GB | ‚úÖ YES | ‚ö°‚ö°‚ö°‚ö° Very Fast | ‚úÖ Good |
| **phi4:14b** | 9.1GB | ‚úÖ YES | ‚ö°‚ö° Moderate | ‚ö†Ô∏è Large |

## üéØ Recommended Models for Your Use Case

### Best Balance (Function Calling + Performance):
1. **qwen3:4b** (2.5GB) - ‚úÖ **CURRENT CHOICE**
   - Fast, supports tools, good for kids' content
   - Memory: ~3.5GB total

2. **llama3.1:8b** (4.7GB) - ‚úÖ **BEST LLAMA OPTION**
   - Official Llama with function calling
   - Memory: ~5.5GB total
   - Better reasoning than qwen3:4b

3. **phi3:3.8b** (2.3GB) - ‚úÖ Good alternative
   - Small, fast, supports tools
   - Memory: ~3.3GB total

### If You Want Llama Specifically:
**Use `llama3.1:8b`** - This is the smallest Llama model with function calling support.

## üì• How to Download Models

```bash
# Download llama3.1:8b (recommended Llama with function calling)
ollama pull llama3.1:8b

# Or download qwen2.5:7b (excellent alternative)
ollama pull qwen2.5:7b

# Or download phi3:3.8b (smaller alternative)
ollama pull phi3:3.8b
```

## üîß How to Switch Models

### Option 1: Use llama3.1:8b (Best Llama)
```bash
# In .env file
LLM_MODEL=llama3.1:8b
OLLAMA_MODEL=llama3.1:8b
```

**Expected Memory:** ~5.5GB total (2.5GB Whisper + 3GB model)

### Option 2: Keep qwen3:4b (Current - Good)
```bash
# In .env file (already set)
LLM_MODEL=qwen3:4b
OLLAMA_MODEL=qwen3:4b
```

**Expected Memory:** ~3.5GB total (2.5GB Whisper + 1GB model)

### Option 3: Use phi3:3.8b (Smallest with tools)
```bash
# In .env file
LLM_MODEL=phi3:3.8b
OLLAMA_MODEL=phi3:3.8b
```

**Expected Memory:** ~3.3GB total (2.5GB Whisper + 800MB model)

## ‚ö†Ô∏è Why llama3.2:3b Doesn't Work

The `llama3.2` series (1b, 3b) are **lightweight models** that don't support function calling:
- ‚ùå No tool use capability
- ‚ùå Will output JSON as text instead of calling functions
- ‚ùå MCP device control won't work

The `llama3.1` series (8b, 70b) are **full models** with function calling:
- ‚úÖ Proper tool use support
- ‚úÖ Can call MCP functions
- ‚úÖ Device control works

## üß™ Test Function Calling

After switching models, test with:
```bash
python test_ollama.py
```

Or test manually:
```bash
curl http://192.168.1.114:11434/api/chat -d '{
  "model": "llama3.1:8b",
  "messages": [{"role": "user", "content": "Set volume to 50"}],
  "tools": [{
    "type": "function",
    "function": {
      "name": "set_device_volume",
      "description": "Set device volume",
      "parameters": {
        "type": "object",
        "properties": {
          "volume": {"type": "integer"}
        }
      }
    }
  }]
}'
```

If the model supports tools, it will return:
```json
{
  "message": {
    "tool_calls": [{
      "function": {
        "name": "set_device_volume",
        "arguments": {"volume": 50}
      }
    }]
  }
}
```

If it doesn't support tools, it will return text like:
```json
{
  "message": {
    "content": "{\"name\": \"set_device_volume\", \"parameters\": {\"volume\": 50}}"
  }
}
```

## üìä Memory Comparison

| Configuration | Whisper | LLM | Total | Function Calling |
|---------------|---------|-----|-------|------------------|
| **Current (qwen3:4b + base.en)** | 2.5GB | 1.0GB | 3.5GB | ‚úÖ YES |
| **llama3.1:8b + base.en** | 2.5GB | 3.0GB | 5.5GB | ‚úÖ YES |
| **llama3.2:3b + base.en** | 2.5GB | 0.8GB | 3.3GB | ‚ùå NO |
| **phi3:3.8b + base.en** | 2.5GB | 0.8GB | 3.3GB | ‚úÖ YES |

## üéØ Final Recommendation

### For Your 8GB RAM System:

**Option 1: Keep qwen3:4b (Current)**
- ‚úÖ Already working
- ‚úÖ Low memory (3.5GB)
- ‚úÖ Fast
- ‚úÖ Function calling works
- ‚ö†Ô∏è Not a Llama model

**Option 2: Switch to llama3.1:8b**
- ‚úÖ Official Llama with function calling
- ‚úÖ Better reasoning
- ‚ö†Ô∏è Higher memory (5.5GB)
- ‚ö†Ô∏è Slower than qwen3:4b

**Option 3: Switch to phi3:3.8b**
- ‚úÖ Smallest with function calling
- ‚úÖ Low memory (3.3GB)
- ‚úÖ Fast
- ‚ö†Ô∏è Not a Llama model

## üöÄ Quick Fix Commands

### To use llama3.1:8b (Best Llama):
```bash
# 1. Download model
ollama pull llama3.1:8b

# 2. Update .env
sed -i 's/LLM_MODEL=qwen3:4b/LLM_MODEL=llama3.1:8b/' .env
sed -i 's/OLLAMA_MODEL=qwen3:4b/OLLAMA_MODEL=llama3.1:8b/' .env

# 3. Restart agent
python simple_main.py
```

### To use phi3:3.8b (Smallest with tools):
```bash
# 1. Download model
ollama pull phi3:3.8b

# 2. Update .env
sed -i 's/LLM_MODEL=qwen3:4b/LLM_MODEL=phi3:3.8b/' .env
sed -i 's/OLLAMA_MODEL=qwen3:4b/OLLAMA_MODEL=phi3:3.8b/' .env

# 3. Restart agent
python simple_main.py
```

## ‚úÖ Summary

- **llama3.2:3b** ‚ùå No function calling
- **llama3.1:8b** ‚úÖ Has function calling (recommended Llama)
- **qwen3:4b** ‚úÖ Has function calling (current, good choice)
- **phi3:3.8b** ‚úÖ Has function calling (smallest option)

Choose based on your priorities:
- **Want Llama?** ‚Üí Use `llama3.1:8b`
- **Want low memory?** ‚Üí Keep `qwen3:4b` or use `phi3:3.8b`
- **Want best quality?** ‚Üí Use `llama3.1:8b`
